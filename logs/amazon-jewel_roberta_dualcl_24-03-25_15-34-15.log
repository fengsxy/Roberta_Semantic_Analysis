> creating model roberta
> cuda memory allocated: 499891712
> training arguments:
>>> data_dir: data
>>> dataset: amazon-jewel
>>> model_name: roberta
>>> method: dualcl
>>> train_batch_size: 16
>>> test_batch_size: 64
>>> num_epoch: 100
>>> lr: 1e-05
>>> decay: 0.01
>>> alpha: 0.5
>>> temp: 0.1
>>> backend: False
>>> timestamp: 1711406055408
>>> device: cuda
>>> num_classes: 2
>>> log_name: amazon-jewel_roberta_dualcl_24-03-25_15-34-15.log
1/100 - 1.00%
[train] loss: 1.5140, acc: 88.62
[test] loss: 2.0487, acc: 91.76, f1: 0.86
[val] loss: 2.0436, acc: 91.73, f1: 0.86
2/100 - 2.00%
[train] loss: 1.3311, acc: 91.82
[test] loss: 2.0119, acc: 92.63, f1: 0.89
[val] loss: 2.0099, acc: 92.65, f1: 0.89
3/100 - 3.00%
[train] loss: 1.2961, acc: 92.97
[test] loss: 2.0017, acc: 92.64, f1: 0.88
[val] loss: 1.9979, acc: 92.67, f1: 0.88
